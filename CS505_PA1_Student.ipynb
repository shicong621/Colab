{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shicong621/Colab/blob/main/CS505_PA1_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this piece of code, we are going to practice scraping data from various sources, i.e., twitter, wikipedia and news websites.\n",
        "\n",
        "**You are going to use the output of the Twitter part to finish your assignment 1 . The rest are going to be used later.**\n",
        "\n",
        "First, we are going to learn how to scrap tweets from Twitter.\n",
        "\n",
        "To do so, you need to sign up for a developer account [here](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api).\n",
        "\n",
        "After doing so, please make sure you have access to your Bearer token as well as other keys/tokens. Otherwise you won't be able to scrap data from Twitter."
      ],
      "metadata": {
        "id": "A2KAxtn4YgN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, you may want to mount your Google Drive so that you could save the scraped data into your drive for later."
      ],
      "metadata": {
        "id": "COJJ6SItqr3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J0UlPqSlqshR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff057bf-ec4f-48a4-a354-f6608f1338f7"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We strongly recommend you to have a look at this [article](https://dev.to/twitterdev/a-comprehensive-guide-for-using-the-twitter-api-v2-using-tweepy-in-python-15d9) \n",
        "to get familiar with the things you can do with scrapping tool.\n",
        "\n",
        "If your problem still exist, don't hesitate to post question(s) on Piazza or come to the TA's office hours."
      ],
      "metadata": {
        "id": "6C3xtTN1lYps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: installing tweepy, the tool to scrap data from Twitter (before that, please make sure you have your Bearer ready)\n",
        "!pip3 install tweepy\n",
        "!pip3 install tweepy --upgrade # make sure your tweepy is up-to-date (>=4.10.1), otherwise there's a chance your won't be able to interact with Twitter API v2.\n",
        "#Restart Runtime might be needed."
      ],
      "metadata": {
        "id": "ojbA8KQiYiWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: initiate your client\n",
        "# You will need to get your bearer token from the email sent to you.\n",
        "import tweepy\n",
        "client = tweepy.Client(bearer_token='YOUR BEARER TOKEN HERE') # replace with your bearer token here.\n"
      ],
      "metadata": {
        "id": "L8NN99MMPWhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: a first try on scraping data, run the following code to see if we could collect some tweets related to 'football'\n",
        "query = 'football lang:en -is:retweet' # the query restricts the collected tweets to contain 'football', to be in English, and to not be re-tweets. \n",
        "tweets = client.search_recent_tweets(query=query, max_results=100) # we are using search_recent_tweets, to search for tweets in recent 7 days. Bring 10 tweets back.\n",
        "for tweet in tweets.data[:10]:\n",
        "    print(tweet.text)\n",
        "    print('-----------------------------')"
      ],
      "metadata": {
        "id": "ka0iOvLXViS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please notice Twitter API has a limit on how many times a user can send request to their data every 15 minutes. So please don't run the code too many times at the same time or use For/While loop to execute the above code. \n",
        "\n",
        "If you can see some tweets printed out, that means your scrapping tool **tweepy** is successfully set up. \n",
        "\n",
        "Before moving on, please read the following links to know how the above function work and how to write a specific query.\n",
        "\n",
        "[query with Twitter API](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query)\n",
        "\n",
        "[search_recent_tweets](https://docs.tweepy.org/en/latest/client.html#tweepy.Client.search_recent_tweets)"
      ],
      "metadata": {
        "id": "hYhKJ9MmoWhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The maximum tweets we could scrap from running the function once is 10-100. To allow scrapping for more data, we could run pagination."
      ],
      "metadata": {
        "id": "C4yaxlbZp8sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'football -is:retweet'\n",
        "tweets = list(tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=1000))\n",
        "print(\"{} tweets are collected.\".format(len(tweets)))"
      ],
      "metadata": {
        "id": "_oJAQTSmreW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more features with the scrapping tool, please refer to the [article](https://dev.to/twitterdev/a-comprehensive-guide-for-using-the-twitter-api-v2-using-tweepy-in-python-15d9). Remember there are some features unavailable to your account (Availability: Essential)."
      ],
      "metadata": {
        "id": "tcMOELa4tyhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let's save our scrapped tweets into a file.\n",
        "\n",
        "If you are using Google Colab, first, mount your Google Drive with the icon on the left hand side.\n",
        "\n",
        "Then, locate the path to save by right clicking the folder to save -> copy path.\n",
        "\n",
        "Change the value of 'driveFolderDirectory' below with your copied path."
      ],
      "metadata": {
        "id": "duDUCGNyw-TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "driveFolderDirectory = '/content/drive/MyDrive/Colab Notebooks/' # if your are not using Google Colab, edit the value directly here.\n",
        "savedFileName = 'tweets.csv'\n",
        "pathToSave = driveFolderDirectory + savedFileName\n",
        "\n",
        "with open(pathToSave, 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx','tweetId', 'tweetText']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i,tweet in enumerate(tweets):\n",
        "    writer.writerow({'idx': i, 'tweetId': tweet.id,'tweetText': tweet.data['text']})\n"
      ],
      "metadata": {
        "id": "f6XFAFDcvKAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's test if you know what your are doing. Get 10000 tweets mentioned 'nft' **written in English**. The tweet should **not be a re-tweet** and there is **no link** in it. Save the them in a .csv file with their **tweet id**, **created time**, and **tweet text**."
      ],
      "metadata": {
        "id": "RVell1Cb3rhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPLEMENT YOUR CODE  HERE #\n",
        "pass"
      ],
      "metadata": {
        "id": "-loUP25IM4ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are going to learn how to scrap data from a Wikipedia page:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Non-fungible_token\n",
        "\n",
        "With requests and BeautifulSoup.\n",
        "\n"
      ],
      "metadata": {
        "id": "RvL8BW4bjj-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's installed the required libraries, i.e. requests and BeautifulSoup\n",
        "!pip install requests\n",
        "#BeautifulSoup should have been installed in Google Colab, if not please run: !pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "NYp7clxskNBz",
        "outputId": "d4c3ad86-7166-415f-b40f-994508c483a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since you already have some experience in scraping data, this time I will leave some useful reading materials and a skeleton code for you to fill in and scrap the following data:\n",
        "\n",
        "First, scrape the \n",
        "contents of this article in Wikipedia:\n",
        "https://en.wikipedia.org/wiki/Pok%C3%A9mon. By saying \"contents\", we only refer to those texts that explains the idea of the title of the page, i.e. the \"article\" part of the webpage. See https://en.wikipedia.org/wiki/Wikipedia:What is an article%3F for what is defined as an article in Wikipedia. (We do not require you to scrap table(s) in the wikipedia at this point, but you should think about how to scrap them from the webpage as well, and how they can benefit training your language model later on.)\n",
        "\n",
        "Second, scrape the contents of all articles within Wikipedia that are linked from only the content of this page i.e., you\n",
        "don’t need to scrape the sidebar—you will have to look at the retrieved HTML of the first page and see the pattern\n",
        "you can use to obtain links from this article’s content to other Wikipedia articles.\n",
        "\n",
        "**Useful Materials:**\n",
        "\n",
        "To get started, please read through the following materials to get an idea how to scrap data from a certain webpage.\n",
        "\n",
        "If you are not familiar with HTML file or its format, please quickly go through the tutorial in [W3School](https://www.w3schools.com/html/).\n",
        "\n",
        "An [introduction](https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/) of how to scrap and parse information from a webpage.\n",
        "\n",
        "[BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) (helpful when you want to know how to use certain functions in BeautifulSoup)\n",
        "\n",
        "Please remember to set a delay time (>=1s) between each request to scrap a webpage, to avoid getting banned from Wikipedia for scraping too fast that could break their server. "
      ],
      "metadata": {
        "id": "unVu3GNFtEDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First, import the needed libraries for scraping data from Wikipedia webpages.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time # for setting up a delay on getting htmls from wiki server.\n",
        "from tqdm import tqdm\n",
        "\n",
        "# First, get the page info from wiki server given an URL.\n",
        "def getPageFromWiki(url):\n",
        "    # get URL\n",
        "    page = requests.get(url)\n",
        " \n",
        "    # scrape webpage\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "# Second, get the title of the wiki page\n",
        "def getHeading(soup):\n",
        "    heading = soup.find('title').text\n",
        "    return heading\n",
        "\n",
        "# Third, get the article part of the wiki page \n",
        "def getContent(page):\n",
        "    content = []\n",
        "    texts = page.find_all('p')\n",
        "    for text in texts:\n",
        "        content.append(text.get_text())\n",
        "    return content\n",
        "\n",
        "# Fourth, get the links that the article part mentioned and specifically, linking to other wiki pages.\n",
        "def getLinks(page):\n",
        "    linksDict = {}\n",
        "\n",
        "    links = page.find_all('a', href = True, title = True)\n",
        "    for link in links:\n",
        "        if (not link.get('href').startswith('https://')):\n",
        "            url = 'https://en.wikipedia.org' + link.get('href')\n",
        "            linksDict[link.get('title')] = url\n",
        "        else:\n",
        "            url = link.get('href')\n",
        "            linksDict[link.get('title')] = url\n",
        "            \n",
        "    return linksDict"
      ],
      "metadata": {
        "id": "pJA2RWPfSSTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once you've implemented the above functions, run the following piece to see if a dictionary that contains the wiki articles we scraped.\n",
        "# Run a for loop to get all the article contents from Wikipedia.\n",
        "pageDict = {}\n",
        "\n",
        "page = getPageFromWiki('https://en.wikipedia.org/wiki/Pok%C3%A9mon') # scrap the main page we want. \n",
        "header = getHeading(page)\n",
        "content = getContent(page)\n",
        "pageDict[header] = content\n",
        "print(pageDict)\n",
        "\n",
        "linksDict = getLinks(page) # get the links contained in the article part of the page.\n",
        "print(\"a set of {} links are found.\".format(len(linksDict)))\n",
        "\n",
        "for title in tqdm(list(linksDict.keys())): # set up a loop to , set a delay at each iteration\n",
        "  url = linksDict[title]\n",
        "  page = getPageFromWiki(url)\n",
        "  header = getHeading(page)\n",
        "  content = getContent(page)\n",
        "  pageDict[header] = content\n",
        "  time.sleep(1) # Remember to set a delay >=1 second so you won't break the server.\n",
        "\n",
        "print(\"a size of {} content dictionary is built.\".format(len(pageDict)))"
      ],
      "metadata": {
        "id": "Ivz07zWjOrwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lastly, save your contents and corresponding title in a .csv file.\n",
        "import csv\n",
        "\n",
        "driveFolderDirectory = '/content/drive/MyDrive/Colab Notebooks/' # if your are not using Google Colab, edit the value directly here.\n",
        "savedFileName = 'wikiContents.csv'\n",
        "pathToSave = driveFolderDirectory + savedFileName\n",
        "\n",
        "with open(pathToSave, 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx','wikiTitle', 'wikiContents']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i,wikiContentKey in enumerate(pageDict.keys()):\n",
        "    writer.writerow({'idx': i, 'wikiTitle': wikiContentKey,'wikiContents': pageDict[wikiContentKey]})"
      ],
      "metadata": {
        "id": "ch1q__ZDe1WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's use what we learn from scraping data in wikipedia webpages to scrap news from news websites, i.e. ABC news and Fox news.\n",
        "\n",
        "In this assignment, you need to get 100 news from each news source. The first thing to consider is to find out where we can get the 100 news URLs. If you've read the [article](https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/) in the wikipedia section, you will know reading the robots.txt of each news website is very helpful. Take a look at the robots.txt from both ABC news and Fox News. You will see that sitemap contains news URLs in the format of XML. You could scrap these sitemaps by BeautifulSoup."
      ],
      "metadata": {
        "id": "7OMmbtL4PQUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getPageFromWiki(url):\n",
        "    # get URL\n",
        "    page = requests.get(url)\n",
        " \n",
        "    # scrape webpage\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    return soup\n"
      ],
      "metadata": {
        "id": "8fmkzC1KPeRt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# def getPageFromWiki(url): # you may use the getPageFromWiki you implemented from the wikipedia section\n",
        "\n",
        "abcNewsSitemap = getPageFromWiki('https://abcnews.go.com/xmlLatestStories')\n",
        "\n",
        "# A similar one can be found in the Fox news robots.txt, here we will leave it to you to find out what the sitemap URL is.\n",
        "foxNewsSitemap = getPageFromWiki('https://www.foxbusiness.com/sitemap.xml?type=news') # FILL THE SITEMAP URL HERE.\n"
      ],
      "metadata": {
        "id": "srSjdrJT3YbZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "pz8pSjq5o--p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import xml.etree.ElementTree as et\n",
        "\n",
        "with urllib.request.urlopen('https://www.foxbusiness.com/sitemap.xml?type=news') as url:\n",
        "    data = url.read()\n",
        "\n",
        "xml = et.fromstring(data)\n",
        "nsmp = {\"doc\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
        "       \n",
        "foxUrlList = [] \n",
        "\n",
        "for url in xml.findall('doc:url', namespaces = nsmp):\n",
        "   loc = url.find('doc:loc', namespaces = nsmp).text\n",
        "  \n",
        "   foxUrlList.append(loc)"
      ],
      "metadata": {
        "id": "oJ5BL7bOxv6r"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with urllib.request.urlopen('https://abcnews.go.com/xmlLatestStories') as url:\n",
        "    data = url.read()\n",
        "\n",
        "xml = et.fromstring(data)\n",
        "nsmp = {\"doc\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
        "       \n",
        "abcUrlList = [] \n",
        "\n",
        "for url in xml.findall('doc:url', namespaces = nsmp):\n",
        "   loc = url.find('doc:loc', namespaces = nsmp).text\n",
        "  \n",
        "   abcUrlList.append(loc)"
      ],
      "metadata": {
        "id": "cJCmJ5UPYOfr"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"foxUrlList\",foxUrlList[:100])"
      ],
      "metadata": {
        "id": "5kTuOomcyXk2",
        "outputId": "b10379cc-3fde-4df9-aa4f-000bc175b052",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foxUrlList ['https://www.foxbusiness.com/media/larry-kudlow-work-hard-preserve-freedom', 'https://www.foxbusiness.com/politics/google-quietly-backtracks-racial-quota-phd-fellowship-program-report', 'https://www.foxbusiness.com/markets/unitedhealth-completes-change-healthcare-merger-court-rejected-doj-challenge', 'https://www.foxbusiness.com/lifestyle/taco-bell-partners-pete-davidson-new-ad-the-apology', 'https://www.foxbusiness.com/media/fox-business-network-announces-new-season-fbn-prime-mark-15th-anniversary', 'https://www.foxbusiness.com/markets/ceos-see-tougher-recession-kpmg', 'https://www.foxbusiness.com/markets/ceos-warn-tough-recession-coming-home-sales-lower-more-tuesdays-5-things-know', 'https://www.foxbusiness.com/lifestyle/hurricane-ian-insurance-crime-experts-warning-car-buyers-flood-damaged-vehicles', 'https://www.foxbusiness.com/politics/energy-experts-sound-alarm-europes-energy-crisis-clear-present-warning-america', 'https://www.foxbusiness.com/markets/boeings-737-max-10-approved-faa-summer-2023-report', 'https://www.foxbusiness.com/politics/biden-must-do-right-now-fix-broken-economy', 'https://www.foxbusiness.com/personal-finance/refinance-student-loan-save-money', 'https://www.foxbusiness.com/politics/arizona-ag-announces-85-million-google-settlement-location-privacy-lawsuit', 'https://www.foxbusiness.com/economy/job-openings-unexpectedly-fall-august-lowest-level-june-2021', 'https://www.foxbusiness.com/lifestyle/ford-sales-fell-september-july-august-results', 'https://www.foxbusiness.com/economy/hurricane-ian-recovery-over-435k-florida-customers-still-without-power', 'https://www.foxbusiness.com/politics/u-s-treasury-seeks-phased-g7-oil-sanctions-eu-ban-looms', 'https://www.foxbusiness.com/lifestyle/pope-francis-apple-ceo-tim-cook-meet-private-vatican-sit-down', 'https://www.foxbusiness.com/lifestyle/dallas-star-patrick-duffy-lists-14m-oregon-ranch-bass-pond-pool-house-wine-cave', 'https://www.foxbusiness.com/personal-finance/todays-mortgage-rates-october-4-2022', 'https://www.foxbusiness.com/lifestyle/national-cinnamon-roll-day-cinnabons-buy-one-get-one-free-offer-available-3-days', 'https://www.foxbusiness.com/personal-finance/todays-mortgage-refinance-rates-october-4-2022', 'https://www.foxbusiness.com/politics/uk-prime-minister-liz-truss-faces-turbulence-as-economic-turmoil-continues', 'https://www.foxbusiness.com/retail/affirm-bringing-consumers-spending-power-back-economic-turbulence-ceo', 'https://www.foxbusiness.com/business-leaders/buffetts-successor-buys-70m-berkshire-stock', 'https://www.foxbusiness.com/personal-finance/housing-market-economy-recession-2023', 'https://www.foxbusiness.com/lifestyle/faa-extends-rest-periods-flight-attendants', 'https://www.foxbusiness.com/markets/micron-plans-100b-computer-chip-factory-new-york', 'https://www.foxbusiness.com/politics/stuart-varney-americans-paying-bidens-green-dreams', 'https://www.foxbusiness.com/lifestyle/smashburger-debuts-chicken-wings-chain-expands-non-burger-offerings', 'https://www.foxbusiness.com/politics/gen-jack-keane-us-investment-ukraine-denied-putin-ambitions-kept-americas-future-secure', 'https://www.foxbusiness.com/money/requested-late-filing-tax-extension-deadline-looming-closer', 'https://www.foxbusiness.com/technology/nasa-spacex-launch-crew-5-astronauts', 'https://www.foxbusiness.com/economy/usps-workers-arrested-1-3m-credit-card-fraud-identity-theft-scheme', 'https://www.foxbusiness.com/markets/elon-musk-proposes-buy-twitter-original-offer', 'https://www.foxbusiness.com/lifestyle/tupperware-selling-target-stores-nationwide', 'https://www.foxbusiness.com/markets/oil-gas-prices-elevated-ahead-key-opec-meeting', 'https://www.foxbusiness.com/live-news/stock-market-today-october-04-2022', 'https://www.foxbusiness.com/markets/facebook-parent-meta-shrink-some-offices-adapts-hybrid-work', 'https://www.foxbusiness.com/economy/fed-governor-warns-inflation-fight-could-take-some-time', 'https://www.foxbusiness.com/politics/us-electric-vehicle-tax-breaks-rile-asian-european-allies', 'https://www.foxbusiness.com/media/larry-kudlow-biden-wokesters-think-stock-market-rigged-bunch-rich-people', 'https://www.foxbusiness.com/economy/us-oil-industry-leaders-call-biden-take-export-ban-off-the-table', 'https://www.foxbusiness.com/markets/sustainability-platform-esg-funds-not-green-seem', 'https://www.foxbusiness.com/politics/truth-social-now-available-samsung-galaxy-store', 'https://www.foxbusiness.com/economy/five-irs-employees-stole-covid-relief-funds-buy-gucci-trips-las-vegas-justice-department-says', 'https://www.foxbusiness.com/lifestyle/melinda-gates-opens-up-unbelievably-painful-divorce', 'https://www.foxbusiness.com/markets/hurricane-ian-caused-modest-damage-mosaic-facilities', 'https://www.foxbusiness.com/lifestyle/tom-brady-gisele-bundchen-fuel-divorce-rumors-look-back-some-priciest-divorces-sports-history', 'https://www.foxbusiness.com/economy/housing-market-united-states-headed-major-slowdown', 'https://www.foxbusiness.com/politics/americans-should-prepare-gas-prices-keep-rising-analysts-warn', 'https://www.foxbusiness.com/markets/opec-output-cut-on-tap-adp-report-wednesdays-5-things-to-know', 'https://www.foxbusiness.com/politics/sen-marshall-introduces-bills-designed-prevent-irs-overreach-following-agency-expansion', 'https://www.foxbusiness.com/live-news/stock-market-news-today-september-30-2022', 'https://www.foxbusiness.com/economy/mortgage-activity-hits-25-year-low-as-rates-rise', 'https://www.foxbusiness.com/sports/dolphins-tyreek-hill-simple-financial-reason-choosing-play-miami', 'https://www.foxbusiness.com/economy/us-companies-added-208000-jobs-september-beating-expectations-adp', 'https://www.foxbusiness.com/politics/rnc-chair-rips-google-alleged-email-suppression-keep-big-techs-thumb-scale-democracy', 'https://www.foxbusiness.com/personal-finance/personal-loans-for-moving-costs', 'https://www.foxbusiness.com/markets/eu-hits-russia-news-sanctions-issues-price-cap-oil-nord-stream-sabotage', 'https://www.foxbusiness.com/entertainment/tom-brady-gisele-bundchens-multimillion-dollar-divorce-hire-attorneys', 'https://www.foxbusiness.com/markets/opec-ministers-agree-cut-production-2m-barrels-per-day', 'https://www.foxbusiness.com/personal-finance/todays-mortgage-rates-october-5-2022', 'https://www.foxbusiness.com/personal-finance/todays-mortgage-refinance-rates-october-5-2022', 'https://www.foxbusiness.com/technology/elon-musk-twitter-purchase-accelerant-creating-x-everything-app', 'https://www.foxbusiness.com/politics/americas-corrupt-president-successfully-manipulated-justice-department-newt-gingrich', 'https://www.foxbusiness.com/politics/kellyanne-conway-rips-entire-dem-party-for-pushing-racial-equity-agenda-everywhere-except-school-choice', 'https://www.foxbusiness.com/politics/lawmakers-criticize-us-reliance-foreign-oil-opec-slashes-production', 'https://www.foxbusiness.com/markets/hurricane-ian-recovery-efforts-boost-procuream-fema-etf', 'https://www.foxbusiness.com/politics/us-steps-away-flagship-lithium-project-berkshire', 'https://www.foxbusiness.com/energy/top-eu-official-vows-stress-test-pipelines-after-leaks', 'https://www.foxbusiness.com/politics/manchin-opec-decision-cut-oil-production-shows-us-must-emphasize-energy-independence-security', 'https://www.foxbusiness.com/politics/stuart-varney-opec-meeting-big-deal-amid-war-global-energy-crisis', 'https://www.foxbusiness.com/politics/uk-truss-stands-by-disruption-agenda-despite-tory-doubts', 'https://www.foxbusiness.com/technology/nasa-spacex-crew-5-mission-launches-space-station', 'https://www.foxbusiness.com/markets/remote-work-boost-productivity-helps-fight-inflation-despite-business-leaders-refusal-accept', 'https://www.foxbusiness.com/politics/louisiana-divests-blackrock-esg-policies-destroy-louisiana-economy', 'https://www.foxbusiness.com/politics/biden-release-10m-more-barrels-strategic-petroleum-reserve-november-wake-opec-cuts', 'https://www.foxbusiness.com/small-business/fort-myers-small-businesses-face-work-done-after-hurricane-ian-restaurateur', 'https://www.foxbusiness.com/markets/europes-gas-crisis-deepen-after-winter-drains-reserves', 'https://www.foxbusiness.com/lifestyle/lottery-officials-philippines-say-nothing-suspicious-433-players-win-4m-jackpo', 'https://www.foxbusiness.com/personal-finance/millennial-debt-credit-cards-student-loans', 'https://www.foxbusiness.com/lifestyle/inflation-curb-holiday-travel-plans', 'https://www.foxbusiness.com/economy/wto-warns-darkened-trade-outlook-could-worsen-economic-crises-converge', 'https://www.foxbusiness.com/technology/tesla-remove-more-vehicle-sensors-replace-with-tesla-vision', 'https://www.foxbusiness.com/politics/white-house-pushes-clean-energy-says-us-must-reduce-reliance-foreign-oil-wake-opec-decision', 'https://www.foxbusiness.com/personal-finance/private-student-loan-rates-october-4-2022', 'https://www.foxbusiness.com/markets/twitter-elon-musks-october-17-trial-proceed-judge-says', 'https://www.foxbusiness.com/politics/us-oil-industry-mocks-biden-opec-announces-production-cuts', 'https://www.foxbusiness.com/personal-finance/how-inflation-impacts-car-insurance-premium', 'https://www.foxbusiness.com/technology/ring-home-security-app-hit-major-outages', 'https://www.foxbusiness.com/retail/rei-makes-closing-its-stores-black-friday-permament', 'https://www.foxbusiness.com/personal-finance/zero-premium-medicare-advantage-plans-what-to-know', 'https://www.foxbusiness.com/financials/capital-one-hacker-paige-thompson-not-sentenced-prison-transgender-status-mental-health', 'https://www.foxbusiness.com/media/larry-kudlow-biden-given-powerhouse-title-back-opec', 'https://www.foxbusiness.com/live-news/stock-market-news-today-october-05-2022']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The next step is to get article URLs, you could do so by the 'select' function from BeautifulSoup, but since the roadmap is a .xml file,\n",
        "# Getting this is a little bit different from the wikipedia one. Regardless, the idea should be similar and it won't be too hard for you to find out how to do so.\n",
        "\n",
        "#Try getting 100 news URLs from the sitemap. \n",
        "from time import sleep\n",
        "\n",
        "def getUrlList(sitemap):\n",
        "  # This function should return a list of URLs of news contained in the sitemap page.\n",
        "  url_list = []\n",
        "  for page in sitemap:\n",
        "    r = requests.get(page)\n",
        "    soup = BeautifulSoup(r.content, 'lxml-xml', \n",
        "                         from_encoding=r.content.info().get_param('charset'))\n",
        "    rows = soup.select('tbody tr')\n",
        "\n",
        "    for row in rows:\n",
        "        d = dict()\n",
        "        d['name'] = row.select_one('.source-title').text.strip()\n",
        "        d['allsides_page'] = 'https://abcnews.go.com' + row.select_one('.source-title a')['href']\n",
        "        \n",
        "        url_list.append(d)\n",
        "    \n",
        "    sleep(100)\n",
        "\n",
        "\n",
        "  # IMPLEMENT YOUR CODE HERE: #\n",
        "\n",
        "  return url_list\n"
      ],
      "metadata": {
        "id": "IQZ-4isB63aq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foxUrlList = []\n",
        "abcUrlList = []\n",
        "\n",
        "foxUrlList = getUrlList(foxNewsSitemap)\n",
        "abcUrlList = getUrlList(abcNewsSitemap)\n",
        "\n",
        "# Test here if the list contains the URLs you want.\n",
        "print(\"foxUrlList\",foxUrlList)\n",
        "print(\"abcUrlList\",abcUrlList)\n"
      ],
      "metadata": {
        "id": "pvMNwJ51pxoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we got the URL list, we could start working on extracting the 'article' part of the news within these webpages. One way to do so is mimic the form from the Wikipedia section. Here we introduce a 'shortcut', using a library called 'newspaper' to help us with that. \n",
        "\n",
        "Please have a brief look at their Github Repository, it's very easy to use. We will leave the task of scraping the article part of the news from each URL using 'newspaper' to you, with a template below.\n",
        "\n",
        "The ABC/Fox news might contain less than 100 news in the sitemap page. Try refreshing the sitemap to get different ones until a number of 100 news are collected for each news source, or, try a different sitemap.\n",
        "\n",
        "Again, you may want to set up a delay every time you make a request to the news server."
      ],
      "metadata": {
        "id": "xfG0p-ygCqhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install newspaper\n",
        "!pip install newspaper3k"
      ],
      "metadata": {
        "id": "gQ1hUYVHDKPk",
        "outputId": "e5f92534-4d32-4f84-c9bc-46555d62f55a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.9.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.7)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 70.5 MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "Collecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.6.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2022.6.15)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.8.0)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=20d2321539c1395600bf911ee0c86b8398e8d1605935212349086ebec28c14a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=ee0d63d846ffd27d459e5d622aa982a29941c4cc059e3337c1953aaa686fe007\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=a52dd197e7ff5842f4577cd3388e22dc074e6ccf9193ec9d7aec45376b8e0886\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=9d509a16bfdd83b3585b78f2c2058386e5b80d79b07b9f5e70c4eb209bffdede\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "from tqdm import tqdm\n",
        "\n",
        "#def getNewsDict(url_list):\n",
        "\n",
        "  # Your key should be the news title and value should be the article text of the news.\n",
        "  #newsDict = {}\n",
        "  # IMPLEMENT YOUR CODE HERE:# \n",
        "  \n",
        "  #return newsDict\n",
        "\n",
        "\n",
        "#abcNews = getNewsDict(abcUrlList)\n",
        "#foxNews = getNewsDict(foxUrlList)\n"
      ],
      "metadata": {
        "id": "h6-C0-4eCIsB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "n9LuYqOtO3WF",
        "outputId": "59fb10a4-0b02-4866-b0f5-7431277ae855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_all = []\n",
        "article_title = []\n",
        "abcNews = {}\n",
        "for url in abcUrlList:\n",
        "  article = Article(url, language=\"en\") # en for English\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  article.nlp()\n",
        "  article_all.append(article.text)\n",
        "  article_title.append(article.title)\n",
        "  zip_iterator = zip(article_title, article_all)\n",
        "  abcNews = dict(zip_iterator)\n",
        "  "
      ],
      "metadata": {
        "id": "oesBI8RV37ey"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foxUrlList.remove(\"https://www.foxbusiness.com/politics/us-steps-away-flagship-lithium-project-berkshire\")"
      ],
      "metadata": {
        "id": "FgI-RUClXua4"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_all2 = []\n",
        "article_title2 = []\n",
        "foxNews = {}\n",
        "for url in foxUrlList:\n",
        "  article = Article(url, language=\"en\") # en for English\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  article.nlp()\n",
        "  article_all2.append(article.text)\n",
        "  article_title2.append(article.title)\n",
        "  zip_iterator2 = zip(article_title2, article_all2)\n",
        "  foxNews = dict(zip_iterator2)"
      ],
      "metadata": {
        "id": "rCpILaBtVQqA"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(foxNews)"
      ],
      "metadata": {
        "id": "94v2oAi4YIdv",
        "outputId": "84e71d07-7806-4cb7-c60e-4c7dc0bf01cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(abcNews)"
      ],
      "metadata": {
        "id": "TGPuYrchUQcw",
        "outputId": "c620e51c-bda1-4bfc-92cc-9fcb50a8493c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lastly, write them down in a .csv file for both the abc and fox news. \n",
        "\n",
        "import csv\n",
        "\n",
        "driveFolderDirectory = '/content/drive/MyDrive/Colab Notebooks/' # if your are not using Google Colab, edit the value directly here.\n",
        "savedFileName = 'newsContents.csv'\n",
        "pathToSave = driveFolderDirectory + savedFileName\n",
        "\n",
        "# size check\n",
        "#assert len(abcNews)>=100 and len(foxNews)>=100, \"the size of both news dictionary should be no less than 100. got {} for abc news and {} for fox news instead.\".format(len(abcNews),len(foxNews))\n",
        "\n",
        "with open(pathToSave, 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx','newsSource','newsTitle','newsContents']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i,newsDictKey in enumerate(abcNews.keys()):\n",
        "    writer.writerow({'idx': i,'newsSource':'ABCNews', 'newsTitle': newsDictKey,'newsContents': abcNews[newsDictKey]})\n",
        "  for i,newsDictKey in enumerate(foxNews.keys()):\n",
        "    writer.writerow({'idx': i,'newsSource':'FoxNews', 'newsTitle': newsDictKey,'newsContents': foxNews[newsDictKey]})"
      ],
      "metadata": {
        "id": "sxpqfTjy8nn1"
      },
      "execution_count": 79,
      "outputs": []
    }
  ]
}